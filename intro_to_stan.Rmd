---
title: "Introduction to STAN"
subtitle: "A probabilistic programming language"
author: Songpeng Zu
date: "`r format(Sys.time(), '%d %B %Y')`"
bibliography: ref.bib
urlcolor: blue
fontsize: 10pt
output:
  # pdf_document:
    # keep_tex: true
  beamer_presentation:
    # theme: "CambridgeUS"
    colortheme: "beaver"
    fonttheme: "professionalfonts"
    keep_tex: true
    # incremental: true
    fig_width: 7
    fig_height: 6
    fig_caption: true
    df_print: kable
    highlight: tango
    citation_package: natbib
    latex_engine: xelatex
    template: "pandoc/mydefault.latex"
    slide_level: 3
    includes:
      in_header: "pandoc/preamble.tex"
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "")
```

### Content[^1]
[^1]: https://github.com/beyondpie/intro_to_stan

An introduction to STAN, a well-designed and easily used tool, for statistical
modeling. 

* What is STAN.

* How STAN works.

* How to write a STAN script.

* How to run it and analyze the result.

# The big picture

### What is STAN [^2]
[^2]: https://mc-stan.org

* A _programming language_
   + It supports array data structure, while loops, and conditionals. 
   + The syntax much like C++. But no need to worry about C++.
   + STAN itself is written in C++.
   + A model written in STAN needs to be compiled.

* Specifically designed for the _statistical modeling_
   + Vector, matrix and their operations
   + A series of probabilistic functions
   + A series of blocks to describe a statistical model.
   + Support sampling, maximum likelihood estimation and variational inference. 
   
### STAN script
```{stan, output.var = "nouse", eval = FALSE}
// A typical stan script is composed by several modules/blocks.
functions {}
data {
  int N;
  real y[N];
  real<lower=0> sigma_y
}
transformed data {}
parameters {real mu;}
transformed parameters {}
model {
  mu ~ normal(0.0, 1.0);
  for (n in 1:N) { y[n] ~ normal(mu, sigma_y);}
}
generated quantities {
  // unused in the model
  // generate replicated data or monitor convergence
  real square_mu;
  square_mu = mu * mu; }
```

# Behind STAN

### MCMC Sampling in STAN

* _Hamiltonian Monte Carlo (HMC)_ provides a general sampling procedure for
  Bayesian inference: using the derivatives of the density function.
  
* HMC and its adaptive variant the no-U-turn sampler (NUTS) [@hoffman2014no] are
used in STAN. NUTS is the default one.

* You DON'T need to write the derivatives in STAN.

### HMC in STAN

* Sampling goal: $p(\theta | y)$, the posterior distribution given data $y$.
  
* HMC introduces auxiliary momentum variables $\rho$. $p(\rho, \theta) = p(\rho
  | \theta) p(\theta)$. In STAN, $p(\rho|\theta) \sim \mathcal{N}(0, M)$, is independent of $\theta$

* Parameters in HMC [See Chapter 15 in STAN Reference Manual]
  + The discretization time $\epsilon$ [^3] will be automatically optimized during
    warmup to match an acceptance rate parameter $\delta$ (default is 0.8).
    Increasing _$\delta$_ will force the
    sampler to use small step sizes, and increase the effective sample size per
    iteration.
  + The $M^{-1}$ (default a diagonal matrix) is estimated during warmup[^4].
  + Number of steps taken $L$ is dynamically adapted during sampling (and during
    warmup) in NUTS, which is controlled by a predefined parameter _treedepth_.

<!-- * _treedepth_ is an important diagnostic tool for NUTS. -->

<!-- * Hamiltonian trajectory is recorded as another diagnostic way to access which -->
  <!-- iterations encountered divergences of the Hamiltonian from the its initial value. -->

[^3]: STAN also allows step-size jitter, which means "jittered" randomly during
      sampling to avoid poor interactions. Default is 0, producing no jitter.

[^4]: STAN supports _Euclidean HMC_. M could be configured by the user.
	 
### Bounded parameters transformation

Besides auto-tuning the parameters in HMC (NUTS), STAN will transform the
bounded variables to an unconstrained ones in the backend. 

The basis idea is to set a one-to-one transformation $y = f(x)$:
$$ p_Y(y) = p_X(f^{-1}(y)) |det J_{f^{-1}}(y)| $$

### Transformations: examples

* $y = \log(x - a)$ if $x$ has lower bound $a$.
* $y = logit(x) = \log \frac{x}{1-x}$ if $x \in (0, 1)$.
* $y = logit(\frac{x-a}{b -a})$ if $x \in (a, b)$.
* If $x$ is ordered, then $y_k = \log (x_k - x_{k-1})$.
* If $x_k >0, \sum_{k}^{K}x_k = 1$, then $y_k = logit(z_k) - log(\frac{1}{K-k});
  z_k = \frac{x_k}{1 - \sum_{s=1}^{k-1}x_s}, z \in \mathcal{R}^{K-1}$.
* When $x$ is a correlation matrix, find the upper triangular $w$ such that $x =
  w w^{T}$, which can be done Cholesky decomposion. Then an inverted
  transformation is designed on $w$ [See chapter 10 in STAN Referenc Manual for details].

### STAN Math Library
* STAN owns a mathematical library [@carpenter2015stan] named STAN Math Library
  that can automatically get the gradients (not the numerical approximation) and
  support matrix operations ,linear 
  algebra, most common probability functions and so on.
  
* It's a C++, reverse-mode automatic differentiation library. Not limited to
  STAN, and designed to be extensive, efficient, scalable and so on.

### Reverse-Mode Automatic Differentiation: an example

$f(y, \mu, \sigma) = -\frac{1}{2} (\frac{y-\mu}{\sigma})^2 - \log \sigma -
\frac{1}{2} \log 2 \pi$

```{r pressure, echo=FALSE, fig.cap="Expression graph of the normal log densityfunction (carpenter2015stan)", out.width = '60%'}
knitr::include_graphics("figures/autodiff.png")
```
<!-- ![Expression graph of the normal log density function [@carpenter2015stan]](figures/autodiff.png) -->

  
# RUN STAN

### How to use STAN?

* Firstly, we need to write the STAN script ended with .stan to describe the
  data we have, the parameters, and the joint distribution of the parameters and
  the data.

* Secondly, we need to compile and run the codes, and analyze the results.

### Show me an example
Let's consider a simple example. 

* Suppose we have $N$ binary observations $y_{1},
y_{2}, \ldots, y_{N}$. They are the _i.i.d_ samples from a $Bernoulli$
distribution under the parameter $\theta$. 

* Our goal is to infer $\theta$.

### Set up the STAN env in R.
```{r include = TRUE, warning = FALSE, message = FALSE}
library(cmdstanr)
# Note: the cmdstan home path is from my computer.
set_cmdstan_path(path = paste(Sys.getenv("HOME"),
                 "softwares",
                 "cmdstan-2.23.0", sep = "/"))
library(bayesplot)
library(posterior)

cmdstan_path()
cmdstan_version()
```
### STAN script

```{r include = TRUE, warning = FALSE, message = FALSE, cache = TRUE}
bern_mod <- cmdstan_model("bernoulli.stan",
                          ## STAN need to be complied.
                          compile = TRUE)
## show the content in the stan script.
bern_mod$print()
```

### Let's feed it some data {.allowframebreaks}

```{r include = TRUE, results = 'hide'}
bern_data  <- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))
## Run MCMC using the 'sample' method
bern_mcmc <- bern_mod$sample(data = bern_data,
                             seed = 355113,
                             chains = 4,
                             parallel_chains = 2,
                             show_message = FALSE)
```

### Summary of the sampling in STAN

```{r include=TRUE}
## use `posterior` package
bern_mcmc$summary("theta", "mean", "sd" , "rhat",
                  "ess_bulk")
```
### Posterior draws {.allowframebreaks}

```{r include=TRUE, collapse = TRUE}
draws <- bern_mcmc$draws()
str(draws)
## use posterior::as_draws_df to transorm the results
## into data.frame.
```

### How about variational inference?

```{r include = TRUE, results = 'hide'}
bern_vb <- bern_mod$variational(data = bern_data,
                                  seed = 355113,
                                  output_samples = 4000)
```

### How about MLE estimation?

```{r include = TRUE, results = 'hide'}
bern_mle <- bern_mod$optimize(data = bern_data,
                              seed = 355113)

```

### Result Summary

```{r echo = TRUE, results = 'hide', fig.height = 3, warning = FALSE, message = FALSE}
bayesplot_grid(
  mcmc_hist(bern_mcmc$draws("theta"), binwidth = 0.02) +
  vline_at(bern_mle$mle(), size = 1.2),
  mcmc_hist(bern_vb$draws("theta"), binwidth = 0.02) +
  vline_at(bern_mle$mle(), size = 1.2),
  titles = c("MCMC", "VI"),
  xlim = c(0,1))
```

# Materials and Summary

### STAN Materials

* [STAN Functions](https://mc-stan.org/docs/2_24/functions-reference/)
  - Use this as the reference materials. When you want some functions, just search it.
  
* [The STAN Language Sytanx](https://mc-stan.org/docs/2_24/reference-manual/index.html)
  - You can scan this if you want to know the whole picture of STAN syntax.
  
* [The User Guide](https://mc-stan.org/docs/2_24/stan-users-guide/index.html)
  - After the introduction, you could read this document smoothly.
  - Lots of examples cover different statistical modelings.
  - It could be a good material to learn statistical models.
  

### Summary
* STAN is designed as a statistical programming language. 
  - Rich of elements, such as matrix operations, probabilistic functions.
  - The clear structures of the blocks for describing a model.
  - Efficiency: C++ backend, multi-thread support, GPU support and map-reduce support.
  
* STAN provides a general and solid inference framework.
  - Uses _NUTS_ and _HMC_ for sampling.
  - Uses _ADVI_  for variational inference.
  - Use _L-BFGS_ for optimization, such as MLE estimation.
  
* STAN has lots of APIs in both R and Python. For R,
  - _cmdstanr_,  _cmdstan_ for compiling and run the latest STAN.
  - _bayesplot_, _posterior_ for analyzing and visualizing the results.
  - _rstan_ as a united interface but not support the latest STAN.
  - _brms_ and _rstanamrs_ simplify the process of writing STAN scripts.

### Thanks! 

* You can find this presentation at https://github.com/beyondpie/intro_to_stan.

* Any suggestions or Pull Requests are welcome.
